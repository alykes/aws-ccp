## Section 8 - Amazon S3  
One of the main building blocks of AWS and is `infinitely scaling` storage  
Many web sites and AWS Services use S3 as part of their integration.  

### Overview  

**Use Cases**  
- Backup and Storage  
- DR  
- Archive  
- Hybrid cloud storage  
- Application hosting  
- Media Hosting  
- Data Lakes and big data analytics  
- software delivery  
- static websites  

**Buckets**  
S3 allows people to store __objects__ (files)  in **buckets** (directories)  

Buckets must have a globally unique name  
Buckets are defined at the regional level  
S3 looks like a global service but the buckets are created in a region  
Naming convention:  
  - no uppercase letters or underscores  
  - 3 - 63 characters long
  - can't use an IP  
  - must start with a lowercase letter or number  
  - must NOT start with the prefix `xn--`  
  - must not end with the suffix `-s3alias`  

**Objects**  
Objects (files) have a `key`  

The key is the Full path:  
  - eg, `s3://my-bucket/my_file.txt`  
The key is composed of `prefix` + `object_name`  
  - eg, `s3://my-bucket/` + `my_file.txt`  
There is no concept of `directories` within buckets, although the UI displays them as such  
The are just keys with very long names and contain `/`  
Object values are the content of the body:  
  - maximum object size is 5TB or 5000GB  
  - if uploading more than 5GB, you must do a `multi-part upload`  
metadata (list of text key-value pairs - system or user metadata)  
Tags (unicode key-value pairs - up to 10) - useful for security/lifecycle  
Version ID (if versioning is available)  

**Note:**  
Once you create an object in a bucket and try to view it (if it's not public) via the public URL, you will get an `authorisation error` **BUT** if you try to **open** it from the web console, it will give you a `presigned URL`, which is basically a URL with your security token in the link.  


### Amazon S3 - Security  

- **User Based**  
  - `IAM Policies` - which API calls should be allowed for a specific user from IAM  

- **Resource Based**  
  - `Bucket Policies` - bucket wide rules from the S3 console - allows cross account access  
  - `Object Access Control List (ACL)` - finer grain control (can be disabled)  
  - `Bucket Access Control List (ACL)` - less common (can be disabled)  

- The most common way to set up permissions on a bucket is through **Bucket Policies**  

**Note:**  
An IAM principal can access an S3 Object if:  
  - The IAM permissions **ALLOW** it **OR** the resource policy **ALLOWS** it  
    **AND** there's no explicit **DENY** rule   

**Encryption**  
- Encrypt objects in S3 using encryption keys    

**Bucket Policies**
- JSON based policies  
  - **Resource**: Which buckets and objects this policies applies to eg `arn:aws:s3:::examplebucket/*`  
  - **Effect**: This is ALLOW or DENY of the defined `Action`    
  - **Actions**: The set of API to Allow or Deny, eg `s3:GetObject`  
  - **Principal**: The account or user to apply the policy to, eg, `*`  
- Use S3 bucket policy to:  
  - Grant public access to the bucket  
  - Force objects to be encrypted at upload  
  - Grant access to another account (Cross Account)  

**Additional settings for Blocking Public Access**  
- Created by AWS as an extra layer of security to prevent company data leaks  
- Can be set at the **account level**  
- If the bucket should **NEVER** be public, then leave the following on:  
    - Block all public access  
        - granted through new access control lists  
        - granted through any access control lists  
        - granted through new public bucket or access point policies  
        - block cross-account acceess to buckets and objects through any public bucket or access point policies  

**Note:**  
- Making objects public  
  - First step is to make the bucket public. Go to **Permissions** tab, edit `Block public access` and turn it __off__ to allow public access.  
  - Second step is to create a bucket policy, **edit** it and add one via the policy generator to set one up. 
  - After the second step, you will get a warning about objects being made public!  
- If you get a **403 Forbidden** error, make sure the bucket policy allows public reads.  

### Amazon S3 - Static Website Hosting  
- S3 can host static websites and have them accessible on the internet  
- The webist URL will be (depending on the region used)  
  - `http://bucket-name.s3-website-aws-region.amazonaws.com`  
  - `http://bucket-name.s3-website.aws-region.amazonaws.com`  

- This is enabled on the bucket itself, you need to navigate to the `Properties` tab and then down to **Static Website Hosting** and select `Enable`
- Select the index document (homepage)  

- Also ensure that you have made all the content in the bucket publically readable.  


### Versioning  
- You can version your files in S3  
- It has to be enabled at the bucket level  
- Same key, overwrite will change the version, that is, 1,2 or 3 etc  
- It's best practise to version your buckets  
  - Protect against unintended deletes (because you can restore to a version)  
  - Easy roll back to previous version  
- **Notes:**  
  - Any file that isn't versioned (already exists) prior to enabling versioning will have version `null`  
    - You will be able to see this when you `Show versions`  
  - Suspending versioning doesn't delete the previous versions  

- To show the versions in the Object explorer, flick the button that says `Show Versions`  

**Deleting Objects**  
- To revert to a previous version, select the new object -> `Delete` it, thus, going back to an older version.  
- To delete an object without, without `Show versions`, it will actually add a **delete marker** and therefore this can be undone. You can see the marker if you `Show versions`.    
- To get the object back after you have deleted it (added the delete marker), you need to **delete** the **delete marker** through `Show versions`.  
- **NOTE:** It's best to play around with this feature as it isn't exactly intuitive.  

### Replication  
- Must have versioning enabled in source and destination buckets
- CRR (Cross-Region Replication)  
- SRR (Same-Region Replication)  
- Buckets can be in different AWS accounts  
- Copying is asynchronous  
- Must giver proper IAM permissions to S3 for read and write  

- **Use Cases:**  
  - CRR - compliance, lower latency access, replication across accounts.  
  - SRR - log aggregation, live replication between production and test accounts.  

- To set up replication, go to the **origin** bucket, then click on `Management` tab.  
  - Now we need to `Create replication rule` 
  - Select the destination Bucket  
  - Under `IAM role`, select `Create new role`  
  - `Create` the rule  
  - **NOTE:** It will pop up and ask you if you want to copy across existing objects using a one-time batch process. Select whatever you want here but be aware that it doesn't copy existing content unless you tell it to!  

### Storage Classes  
- Standard  
- Standard - Infrequent Access  
- One-Zone - Infrequent Access  
- Glacier Instant Retreival  
- Glacier Flexible Retreival  
- Glacier Deep Archive   
- Intelligent Tiering  

Once you create a storage class, you can move between them manually or using S3 lifecycle configurations  

**Durability and Availability**  
- Durability:  
  - High durability 99.999999999% (11 9's) of objects across multiple AZ  
  - If you store 10,000,000 objects with Amazon S3, you can on average expecte to incur a loss of a single object ones every 10,000 years!  
  - Same for all storage classes  
- Availability:  
  - Measures how readily available a service is  
  - varies depledning on the storage class  
  - eg, s3 standard has 99.99% availability or 53 minutes a year of downtime.  

**S3 Standard - General Purpose**  
- 99.99% Availability  
- Used for frequently accessed data  
- Low latency and high throughput  
- sustains 2 concurrent facility failures  
- Use Case:  
  - Big Data analytics, mobile & gaming apps, content distribution  

**S3 Standard - Infrequent Access**  
- data that is infrequently accessed but still requires rapid access when needed  
- Lower cost than S3 standard  
- But you pay for retrieval  

- **Standard - Infrequent Access (S3 Standard IA)**  
  - 99.9% Availability  
  - Use Case:  
    - DR, backups

- **One-Zone - Infrequent Access (S3 One Zone IA)**  
  - High Durability (11 9's) in a single AZ; data lost if AZ is destroyed  
  - 99.95% Availability  
  - Use Case:  
    - Secondary backups of on-prem data or data you can re-create  

**S3 Glacier Storage Classes**
- Low-cost object storage meant for backups and archiving  
- pay for storage plus retreival costs  

- **Glacier Instant Retreival**  
  - ms retrieval, good for data accessed once per quarter  
  - min storage duration of 90 days  

- **Glacier Flexible Retreival - formerly Amazon S3 Glacier**
  - retrieval tiers: expedited (1 to 5 mins), standard (3 to 5 hours), Bulk (5 to 12 hours but is free)  
  - min storage duration of 90 days  

- **Glacier Deep Archive**   
  - retrieval tiers: Standard (12 hours), Bulk (48 hours)  
  - min storage duration of 180 days 

- **Intelligent Tiering**  
  - Small monthly monitoring and auto-tiering fee  
  - Moves objects automatically between Access Tiers based on usage  
  - No retrieval charges  

Frequent Access tier (auto): default  
Infrequent Access tier (auto): object not accessed for 30 days  
Archive Instant Access tier (auto): object not accessed for 90 days  
Archive Access tier (optional): configurable from 90 to 700+ days  
Deep Archive Access tier (optional): configurable from 180 to 700+ days  

**You select the tier when you upload files to S3**  
**You can also change the storage class on files**  

- **Lifecycle Management Rules**  
To set up rules to manage your data movement between storage classes.  
- Go to the bucket and then select the `Management` tab  
- Click `Create Lifecycle Rule`  
  - You can limit the scope using filters, or apply to all objects in the bucket  
  - Select the rule actions - eg, move current versions of objects between storage classes  
  - Create the rule!  

### S3 Encryption  
**Server Side Envryption (Default)**  
When it arrives in the bucket, Amazon S3 encrypts it  
**Client Side Encryption**  
When the user encrypts the file before uploading it to S3  

### Shared Responsibility Model for S3  
AWS:  
  - Infra (security, durability, availability, sustain concurrent loss of two facilities)  
  - configuration and vuln analysis  
  - compliance validation  
Customer:  
 - Versioning  
 - Bucket Policies  
 - Logging and Monitoring  
 - Storage Classes are optimally set  
 - Encryption at rest and in-transit  

### AWS Snow Family  
- Highly-secure, protable devices to **collect and process data at the edge** and **migrate data into and out of AWS**  

Data Migration Tools include:  
 - `Snowcone`  
 - `Snowball Edge`  
 - `Snowmobile`  

Edge Computing tools:  
  - `Snowcone`  
  - `Snowball Edge`  

Why would you want to use the Snow Family set of tools to perform the data migrations?  
It can take a lot of time using a network connection  
eg 10TB over 100Mbps takes 12 days
Challenges:  
  - Limited Connectivity  
  - Limited Bandwidth  
  - High Network Cost  
  - Shared bandwidth (can't maximise the line speed)  
  - Connection Stability  

**AWS Snow Family: offline devices to perform data migrations**  
If it takes more than a week to transfer over the network, use the snowball devices!  

Client gets a physical AWS Snowball device, copy onto it, give it back to AWS, they then upload it to S3  

**Snowball Edge**   
  - stores TBs or PBs of data  
  - Pay per data transfer  
  - Provides block storage and Amazon S3-compatible object storage
  - Two flavours:  
    - `Snowball Edge Storage Optimised`  
      - **80TB** of HDD capacity for block volumes and S3 sompatible object storage  
    - `Snowball Edge Compute Optimised`  
      - 42TB of HDD or 28TB NVMe capacity for block volumes and S3 compatible object storage  
  - Use cases:  large data cloud migrations, DC decommission, DR  

**Snowcone & Snowcone SSD**  
 - Small portable computing, anywhere, rugged and secure, withstands harsh environments  
 - Light; 2.1kg  
 - Device used for edge computing, storage and data transfer  
 - `Snowcone` - 8TB of HDD storage 
 - `Snowcone SSD` - 14TB of SSD storage  
 - Use `Snowcone` where `Snowball` does not physically fit  
 - Must provide your own battery and cables  
 - Can be sent back to AWS offline or connect it to the internet and use `AWS DataSync` to send the data  
 - DataSync is pre-installed on the device  

 **Snowmobile**  
 - An actual truck! 
 - For massive amounts of data, as in exabytes or Petabytes  
 - Each Snowmobile has 100PB of capacity (use multiple in parrallel)  
 - High Security: temperature controlled, GPS, 24.7 surveillance  
 - `Exam Tip`: Better than `Snowball Edge` if you transfer more that **10PB** of data!  

**Snow Family - Usage Process**  
1. Request Snowball devices from the AWS console for delivery  
1. Install the snowball client / AWS OpsHub on your servers  
1. Connect the snowball to your servers and copy files using the client  
1. Ship back the device when you're done  
1. Data loaded into an S3 bucket  
1. Snowball device is completely wiped  

**What is Edge Computing?**  
This is when you process data while it is being created at an **Edge Location**  
An edge location is a location: 
  - that doesn't have internet connection or limited connectivity  
  - there is no access to computing power  
You can set up a `Snowball Edge` or `Snowcone` device to perform the edge computing  
- Use Cases:  
  - Pre-process data  
  - Machine Learning at the Edge  
  - Transcoding media streams  

Eventually you can ship the device back to AWS (eg, to transfer the data to S3)  

**Snow Family - Edge Computing**  
- `Snowcone & Snowcone SSD(smaller version)`  
  - 2 CPU, 4GB RAM, wired or wireless
  - USB-C power using a cord or optional battery
- `Snowball Edge - Storage Optimised`  
  - 40 vCPU, 80GiB RAM, 80TB storage  
- `Snowball Edge - Compute Optimised`  
  - Up to 104 vCPU, 416GiB RAM  
  - optional GPU (machine learning or video processing)  
  - 28TB NVMe or 42TB HDD  
  - Storage Clustering available (up to 16 nodes)  
- **ALL**: can run EC2 instances & AWS Lambda functions (using AWS IoT Greengrass)  
- Long-term deployment options are available: 1 and 3 years at discounted pricing  

**AWS OpsHub**  
- Historically to use the Snow Family devices you needed a CLI tool  
- Now you can use AWS OpsHub and install on your computer to manage your Snow Family devices.  
- It comes with a UI  
- You can do the following:  
  - Unlock or configure single or clustered devices  
  - transfer files  
  - launch and manage instances running on Snow Family Devices  
  - Monitor device metrics (storage, capacity, number of instances on the device)  
  - Launch compatible AWS services on your devices (EC2, DataSynd, NFS etc)  

### AWS Storage Gateway  
- AWS is pushing for `Hybrid Cloud`  
  - Where part of your infra is on-prem and on the cloud  
- Could be due to a range of reasons for hybrid  
  - long cloud migrations  
  - security requirements  
  - compliance requirements  
  - IT strategy  
- S3 is a proprietary storage technology (unlike NFS/EFS)
- AWS Storage Gateway allows you to expose the S3 data on-prem  

Native Options  
- **Block** - AWS EBS, EC2 Instance Store  
- **File** - Amazon EFS  
- **Object** -  Amazon S3, Glacier

**AWS Storage Gateway**  
- Bridges the gap between on-prem data and cloud data in S3  
- `Exam Tip`: **Hybrid Storage Service to allow on-prem to seamlessly use the AWS Cloud**  
- Use Cases: 
  - DR, backup & restore, tiered storage  

You don't need to know the types for the exam but they are:  
  - File Gateway  
  - Volume Gateway  
  - Tape Gateway  

## Summary  
Buckets vs Objects:  global unique names, ties to a region  
S3 Security: IAM Policy, S3 Bucket Policy (Public Access), S3 Encryption  
S3 Websites: host a static website  
S3 Versioning: multiple version for files, prevent accidental deletaion  
S3 Replication: same-regions or cross-region, must enable versioning for it to work  
S3 Storage Classes: Standard, IA, 1Z-IA, Intelligent, Glacier(Instant, Flexible and Deep)  
Snow Family: import data into AWS S3 through physical devices and edge computing  
OpsHub: Desktop application to manage Snow Family Devices  
Storage Gateway: Hybrid solution to extend on-prem storage to S3